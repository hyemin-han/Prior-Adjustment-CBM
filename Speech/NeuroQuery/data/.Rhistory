# neurosynth
neurosynth<-img_data(readNIfTI('./transformed_speech_association-test_z_FDR_0.01.nii'))
calc_overlap(neurosynth,bayes_707)[1]
calc_overlap(neurosynth,bayes_adjusted)[1]
neurosynth[abs(neurosynth)<1e-4]<-0
calc_overlap(neurosynth,bayes_adjusted)[1]
calc_overlap(neurosynth,bayes_707)[1]
# brainmap
brainmap<-drop(img_data(readNIfTI('./transformed_speech_brainmap_C01_1k_ALE.nii')))
calc_overlap(brainmap,bayes_adjusted)[1]
calc_overlap(brainmap,bayes_707)[1]
brainmap[abs(brainmap)<1e-4]<-0
calc_overlap(brainmap,bayes_707)[1]
calc_overlap(brainmap,bayes_adjusted)[1]
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Face/Brainmap/data")
library(oro.nifti)
# load ALE file
ALE <- oro.nifti::readNIfTI('face_brainmap_ALE.nii')
ALE.img <- oro.nifti::img_data(ALE)
# load thresholded file
THRES <- oro.nifti::readNIfTI('face_brainmap_C01_1k_ALE.nii')
THRES.img <- oro.nifti::img_data(THRES)
# mask (zeros -> no data)
mask <- ALE.img != 0
mask[mask==0]<-NA
# how many voxels to be considered?
voxels <- sum(!is.na(mask))
# calculate standard deviation
SD <- sd(mask*ALE.img,na.rm = T)
# calculate the mean signal strength in significant voxels
THRES.0 <- mask*THRES.img
mean.1 <- mean(ALE.img[THRES.0!=0],na.rm=T)
# calculate the mean signal strength in non-significant voxels
mean.0 <- mean(ALE.img[THRES.0==0],na.rm=T)
# calculate ratio
R <- sum(THRES.0 !=0, na.rm=T)/voxels
# calculate mean contrast
C <- mean.1-mean.0
# calculate factor
F <- C/SD*R
# print results
print(sprintf('ratio = %f',R))
print(sprintf('contrast = %f',C))
print(sprintf('noise in SD = %f',SD))
print(sprintf('X = %f',F))
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Face/Brainmap")
library(oro.nifti)
library(imager)
library(reticulate)
# calculate overlap index
calc_overlap<-function(original, target){
# same size?
if (sum(dim(original)==dim(target))<3){
# different. error!
return(-1)
}
# calculate overlap index
V_overlap<-0
V_original<-0
V_target<-0
img_size<-dim(original)
# loop
for (x in 1:img_size[1]){
for (y in 1:img_size[2]){
for (z in 1:img_size[3]){
# na?
if (is.na(original[x,y,z])){
next
}
if (is.na(target[x,y,z])){
next
}
# original
if (original[x,y,z]>0){
V_original<-V_original+1
}
# target
if (target[x,y,z]>0){
V_target<-V_target+1
}
#overlap
if ( (original[x,y,z]*target[x,y,z]>0)){
V_overlap<-V_overlap+1
}
}
}
}
return(c(V_overlap*V_overlap/V_original/V_target/(V_overlap/V_original+V_overlap/V_target),
V_overlap,V_original,V_target))
}
# python module for conversion
# HCP related files should be converted
reticulate::source_python('convert_nii_one.py')
convert_nii('./face_Henson_fwe.nii')
convert_nii('./face_association-test_z_FDR_0.01.nii')
convert_nii('./neuroquery.nii')
convert_nii('./face_brainmap_C01_1k_ALE.nii')
convert_nii('./Henson/BFs_80.nii')
convert_nii('./Henson/BFs_85.nii')
convert_nii('./Henson/BFs_90.nii')
convert_nii('./Henson/BFs_95.nii')
#convert_nii('./Bayes_meta.hdr')
convert_nii('./Henson/Bayes_707.nii')
# compare bayesmeta and all
bayes_adjusted<-img_data(readNIfTI('./Henson/transformed_BFs_90.nii'))>=3
bayes_707 <-img_data(readNIfTI('./Henson/transformed_Bayes_707.nii'))>=3
fwe<-img_data(readNIfTI('./transformed_face_Henson_fwe.nii'))
# neurosynth
neurosynth<-img_data(readNIfTI('./transformed_face_association-test_z_FDR_0.01.nii'))
neurosynth[abs(neurosynth)<1e-4]<-0
calc_overlap(neurosynth,bayes_adjusted)[1]
calc_overlap(neurosynth,bayes_707)
calc_overlap(neurosynth,bayes_707)[1]
calc_overlap(neurosynth,fwe)[1]
# brainmap
brainmap<-drop(img_data(readNIfTI('./transformed_speech_brainmap_C01_1k_ALE.nii')))
# brainmap
brainmap<-drop(img_data(readNIfTI('./transformed_face_brainmap_C01_1k_ALE.nii')))
calc_overlap(brainmap,bayes_adjusted)[1]
brainmap[abs(brainmap)<1e-4]<-0
calc_overlap(brainmap,bayes_adjusted)[1]
calc_overlap(brainmap,bayes_707)[1]
calc_overlap(brainmap,fwe)[1]
calc_overlap(neurosynth,bayes_adjusted)
calc_overlap(neurosynth,bayes_707)
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Face_data/Gordon")
dir()
dir('.nii')
dir('*.nii')
test<-dir()
view(test)
View(test)
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Face/Brainmap")
library(oro.nifti)
library(imager)
library(reticulate)
# calculate overlap index
calc_overlap<-function(original, target){
# same size?
if (sum(dim(original)==dim(target))<3){
# different. error!
return(-1)
}
# calculate overlap index
V_overlap<-0
V_original<-0
V_target<-0
img_size<-dim(original)
# loop
for (x in 1:img_size[1]){
for (y in 1:img_size[2]){
for (z in 1:img_size[3]){
# na?
if (is.na(original[x,y,z])){
next
}
if (is.na(target[x,y,z])){
next
}
# original
if (original[x,y,z]>0){
V_original<-V_original+1
}
# target
if (target[x,y,z]>0){
V_target<-V_target+1
}
#overlap
if ( (original[x,y,z]*target[x,y,z]>0)){
V_overlap<-V_overlap+1
}
}
}
}
return(c(V_overlap*V_overlap/V_original/V_target/(V_overlap/V_original+V_overlap/V_target),
V_overlap,V_original,V_target))
}
# python module for conversion
# HCP related files should be converted
reticulate::source_python('convert_nii_one.py')
convert_nii('./face_gordon_fwe.nii')
convert_nii('./face_association-test_z_FDR_0.01.nii')
convert_nii('./neuroquery.nii')
convert_nii('./face_brainmap_C01_1k_ALE.nii')
convert_nii('./Gordon/BFs_80.nii')
convert_nii('./Gordon/BFs_85.nii')
convert_nii('./Gordon/BFs_90.nii')
convert_nii('./Gordon/BFs_95.nii')
#convert_nii('./Bayes_meta.hdr')
convert_nii('./Gordon/Bayes_707.nii')
# compare bayesmeta and all
bayes_adjusted<-img_data(readNIfTI('./Gordon/transformed_BFs_90.nii'))>=3
bayes_707 <-img_data(readNIfTI('./Gordon/transformed_Bayes_707.nii'))>=3
fwe<-img_data(readNIfTI('./transformed_face_gordon_fwe.nii'))
convert_nii('./Gordon/face_gordon_fwe.nii')
fwe<-img_data(readNIfTI('./Gordon/transformed_face_gordon_fwe.nii'))
calc_overlap(bayes_meta,bayes_adjusted)
# neurosynth
neurosynth<-img_data(readNIfTI('./transformed_face_association-test_z_FDR_0.01.nii'))
neurosynth[abs(neurosynth)<1e-4]<-0
calc_overlap(neurosynth,bayes_adjusted)
calc_overlap(neurosynth,bayes_707)
calc_overlap(neurosynth,fwe)
# brainmap
brainmap<-drop(img_data(readNIfTI('./transformed_face_brainmap_C01_1k_ALE.nii')))
brainmap[abs(brainmap)<1e-4]<-0
calc_overlap(brainmap,bayes_adjusted)
calc_overlap(brainmap,bayes_707)
calc_overlap(brainmap,fwe)
# compare bayesmeta and all 95%
bayes_adjusted<-img_data(readNIfTI('./Gordon/transformed_BFs_95.nii'))>=3
calc_overlap(neurosynth,bayes_adjusted)
calc_overlap(brainmap,bayes_adjusted)
convert_nii('./face_neuroquery.nii')
# neuroquery
# needs size up -> do rescale
neuroquery<-img_data(readNIfTI('./face_transformed_neuroquery.nii'))
# neuroquery
# needs size up -> do rescale
neuroquery<-img_data(readNIfTI('./transformed_face_neuroquery.nii'))
neuroquery[abs(neuroquery)<1e-4]<-0
dim(neuroquery)
sum(neuroquery>=3)
calc_overlap(neuroquery>=3,bayes_adjusted)[1]
calc_overlap(neuroquery>=3,bayes_707)[1]
calc_overlap(neuroquery>=3,fwe)[1]
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Face/NeuroQuery/data")
library(oro.nifti)
# load ALE file
ALE <- oro.nifti::readNIfTI('face_neuroquery.nii')
ALE.img <- oro.nifti::img_data(ALE)
# load thresholded file
THRES <- oro.nifti::readNIfTI('face_neuroquery.nii')
THRES.img <- oro.nifti::img_data(THRES)
# thresholding
THRES.img <- THRES.img >= 3
# mask (zeros -> no data)
mask <- ALE.img != 0
mask[mask==0]<-NA
# how many voxels to be considered?
voxels <- sum(!is.na(mask))
# calculate standard deviation
SD <- sd(mask*ALE.img,na.rm = T)
# calculate the mean signal strength in significant voxels
THRES.0 <- mask*THRES.img
mean.1 <- mean(ALE.img[THRES.0!=0],na.rm=T)
# calculate the mean signal strength in non-significant voxels
mean.0 <- mean(ALE.img[THRES.0==0],na.rm=T)
# calculate ratio
R <- sum(THRES.0 !=0, na.rm=T)/voxels
# calculate mean contrast
C <- mean.1-mean.0
# calculate factor
F <- C/SD*R
# print results
print(sprintf('ratio = %f',R))
print(sprintf('contrast = %f',C))
print(sprintf('noise in SD = %f',SD))
print(sprintf('X = %f',F))
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Speech/Brainmap")
library(oro.nifti)
library(imager)
library(reticulate)
# calculate overlap index
calc_overlap<-function(original, target){
# same size?
if (sum(dim(original)==dim(target))<3){
# different. error!
return(-1)
}
# calculate overlap index
V_overlap<-0
V_original<-0
V_target<-0
img_size<-dim(original)
# loop
for (x in 1:img_size[1]){
for (y in 1:img_size[2]){
for (z in 1:img_size[3]){
# na?
if (is.na(original[x,y,z])){
next
}
if (is.na(target[x,y,z])){
next
}
# original
if (original[x,y,z]>0){
V_original<-V_original+1
}
# target
if (target[x,y,z]>0){
V_target<-V_target+1
}
#overlap
if ( (original[x,y,z]*target[x,y,z]>0)){
V_overlap<-V_overlap+1
}
}
}
}
return(c(V_overlap*V_overlap/V_original/V_target/(V_overlap/V_original+V_overlap/V_target),
V_overlap,V_original,V_target))
}
# python module for conversion
# HCP related files should be converted
reticulate::source_python('convert_nii_one.py')
convert_nii('./fwe_HCP.nii')
convert_nii('./speech_association-test_z_FDR_0.01.nii')
convert_nii('./speech_neuroquery.nii')
convert_nii('./speech_brainmap_C01_1k_ALE.nii')
convert_nii('./HCP/BFs_80.nii')
convert_nii('./HCP/BFs_85.nii')
convert_nii('./HCP/BFs_90.nii')
convert_nii('./HCP/BFs_95.nii')
convert_nii('./Bayes_meta.hdr')
convert_nii('./HCP/Bayes_707.nii')
# compare bayesmeta and all
bayes_adjusted<-img_data(readNIfTI('./HCP/transformed_BFs_90.nii'))>=3
bayes_707 <-img_data(readNIfTI('./HCP/transformed_Bayes_707.nii'))>=3
fwe<-img_data(readNIfTI('./transformed_fwe_HCP.nii'))
fwe<-img_data(readNIfTI('./HCP/transformed_fwe_HCP.nii'))
convert_nii('./HCP/speech_hpc_fwe.nii')
fwe<-img_data(readNIfTI('./HCP/transformed_speech_hpc_fwe.nii'))
calc_overlap(bayes_meta,bayes_adjusted)[1]
# neurosynth
neurosynth<-img_data(readNIfTI('./transformed_speech_association-test_z_FDR_0.01.nii'))
calc_overlap(neurosynth,bayes_adjusted)[1]
calc_overlap(neurosynth,bayes_707)[1]
calc_overlap(neurosynth,fwe)[1]
# brainmap
brainmap<-drop(img_data(readNIfTI('./transformed_speech_brainmap_C01_1k_ALE.nii')))
brainmap[abs(brainmap)<1e-4]<-0
result_brainmap<-as.matrix(c(
calc_overlap(brainmap,bayes_adjusted)[1],
calc_overlap(brainmap,bayes_707)[1],
calc_overlap(brainmap,fwe)[1]
))
result_brainmap
# neuroquery
# needs size up -> do rescale
neuroquery<-img_data(readNIfTI('./transformed_speech_neuroquery.nii'))
#neuroquery<-ifelse(neuroquery==0,NA,neuroquery)
neuroquery[abs(neuroquery)<1e-4]<-0
result_neuroquery<-as.matrix(c(
calc_overlap(neuroquery>=3,bayes_adjusted)[1],
calc_overlap(neuroquery>=3,bayes_707)[1],
calc_overlap(neuroquery>=3,fwe)[1]
))
result_neuroquery
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Face/NeuroQuery")
library(oro.nifti)
library(imager)
library(reticulate)
# calculate overlap index
calc_overlap<-function(original, target){
# same size?
if (sum(dim(original)==dim(target))<3){
# different. error!
return(-1)
}
# calculate overlap index
V_overlap<-0
V_original<-0
V_target<-0
img_size<-dim(original)
# loop
for (x in 1:img_size[1]){
for (y in 1:img_size[2]){
for (z in 1:img_size[3]){
# na?
if (is.na(original[x,y,z])){
next
}
if (is.na(target[x,y,z])){
next
}
# original
if (original[x,y,z]>0){
V_original<-V_original+1
}
# target
if (target[x,y,z]>0){
V_target<-V_target+1
}
#overlap
if ( (original[x,y,z]*target[x,y,z]>0)){
V_overlap<-V_overlap+1
}
}
}
}
return(c(V_overlap*V_overlap/V_original/V_target/(V_overlap/V_original+V_overlap/V_target),
V_overlap,V_original,V_target))
}
# python module for conversion
# HCP related files should be converted
reticulate::source_python('convert_nii_one.py')
convert_nii('./Gordon/face_gordon_fwe.nii')
convert_nii('./face_association-test_z_FDR_0.01.nii')
convert_nii('./face_neuroquery.nii')
convert_nii('./face_brainmap_C01_1k_ALE.nii')
convert_nii('./Gordon/BFs_80.nii')
convert_nii('./Gordon/BFs_85.nii')
convert_nii('./Gordon/BFs_90.nii')
convert_nii('./Gordon/BFs_95.nii')
#convert_nii('./Bayes_meta.hdr')
convert_nii('./Gordon/Bayes_707.nii')
# load
bayes_adjusted<-img_data(readNIfTI('./Gordon/transformed_BFs_90.nii'))>=3
bayes_707 <-img_data(readNIfTI('./Gordon/transformed_Bayes_707.nii'))>=3
fwe<-img_data(readNIfTI('./Gordon/transformed_face_gordon_fwe.nii'))
# neurosynth
neurosynth<-img_data(readNIfTI('./transformed_face_association-test_z_FDR_0.01.nii'))
neurosynth[abs(neurosynth)<1e-4]<-0
result_neurosynth<-as.matrix(c(
calc_overlap(neurosynth,bayes_adjusted)[1],
calc_overlap(neurosynth,bayes_707)[1],
calc_overlap(neurosynth,fwe)[1]
))
result_neurosynth
# brainmap
brainmap<-drop(img_data(readNIfTI('./transformed_face_brainmap_C01_1k_ALE.nii')))
brainmap[abs(brainmap)<1e-4]<-0
result_brainmap<-as.matrix(c(
calc_overlap(brainmap,bayes_adjusted)[1],
calc_overlap(brainmap,bayes_707)[1],
calc_overlap(brainmap,fwe)[1]
))
result_brainmap
# needs size up -> do rescale
neuroquery<-img_data(readNIfTI('./transformed_face_neuroquery.nii'))
neuroquery[abs(neuroquery)<1e-4]<-0
#neuroquery<-ifelse(neuroquery==0,NA,neuroquery)
result_neuroquery<-as.matrix(c(
calc_overlap(neuroquery>=3,bayes_adjusted)[1],
calc_overlap(neuroquery>=3,bayes_707)[1],
calc_overlap(neuroquery>=3,fwe)[1]
))
result_neuroquery
# compare bayesmeta and all 80%
bayes_adjusted<-img_data(readNIfTI('./Gordon/transformed_BFs_80.nii'))>=3
# neurosynth
result_neurosynth<-as.matrix(c(
calc_overlap(neurosynth,bayes_adjusted)[1]
))
# brainmap
result_brainmap<-as.matrix(c(
calc_overlap(brainmap,bayes_adjusted)[1]
))
# neuroquery
# needs size up -> do rescale
neuroquery<-ifelse(neuroquery==0,NA,neuroquery)
result_neuroquery<-as.matrix(c(
calc_overlap(neuroquery>=3,bayes_adjusted)[1]
))
# neuroquery
# needs size up -> do rescale
neuroquery<-img_data(readNIfTI('./transformed_face_neuroquery.nii'))
neuroquery[abs(neuroquery)<1e-4]<-0
# compare bayesmeta and all 80%
bayes_adjusted<-img_data(readNIfTI('./Gordon/transformed_BFs_80.nii'))>=3
# neurosynth
result_neurosynth<-as.matrix(c(
calc_overlap(neurosynth,bayes_adjusted)[1]
))
# brainmap
result_brainmap<-as.matrix(c(
calc_overlap(brainmap,bayes_adjusted)[1]
))
# neuroquery
# needs size up -> do rescale
#neuroquery<-ifelse(neuroquery==0,NA,neuroquery)
result_neuroquery<-as.matrix(c(
calc_overlap(neuroquery>=3,bayes_adjusted)[1]
))
result_neurosynth
result_brainmap
result_neuroquery
setwd("~/Documents/Research/Collaboration/Bayes/Adjust/Speech/NeuroQuery/data")
library(oro.nifti)
# load ALE file
ALE <- oro.nifti::readNIfTI('speech_neuroquery.nii')
ALE.img <- oro.nifti::img_data(ALE)
# load thresholded file
THRES <- oro.nifti::readNIfTI('speech_neuroquery.nii')
THRES.img <- oro.nifti::img_data(THRES)
# thresholding
THRES.img <- THRES.img >= 3
# mask (zeros -> no data)
mask <- ALE.img != 0
mask[mask==0]<-NA
# how many voxels to be considered?
voxels <- sum(!is.na(mask))
# calculate standard deviation
SD <- sd(mask*ALE.img,na.rm = T)
# calculate the mean signal strength in significant voxels
THRES.0 <- mask*THRES.img
mean.1 <- mean(ALE.img[THRES.0!=0],na.rm=T)
# calculate the mean signal strength in non-significant voxels
mean.0 <- mean(ALE.img[THRES.0==0],na.rm=T)
# calculate ratio
R <- sum(THRES.0 !=0, na.rm=T)/voxels
# calculate mean contrast
C <- mean.1-mean.0
# calculate factor
F <- C/SD*R
# print results
print(sprintf('ratio = %f',R))
print(sprintf('contrast = %f',C))
print(sprintf('noise in SD = %f',SD))
print(sprintf('X = %f',F))
